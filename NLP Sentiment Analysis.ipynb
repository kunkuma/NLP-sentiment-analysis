{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd05cab",
   "metadata": {},
   "source": [
    "# 0. Pip Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2625920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported opendatasets\n",
      "Successfully imported polars\n",
      "Successfully imported nltk\n",
      "Successfully imported wordcloud\n",
      "Successfully imported stop_words\n",
      "Successfully imported torch\n",
      "Successfully imported transformers\n"
     ]
    }
   ],
   "source": [
    "# Install all requirements\n",
    "\n",
    "packages_to_install = ['opendatasets', 'polars', 'nltk', 'wordcloud', 'stop_words', 'torch', 'transformers']\n",
    "def install_packages(pckg):\n",
    "    try:\n",
    "       __import__(pckg)\n",
    "       print(\"Successfully imported\", pckg)\n",
    "    except ModuleNotFoundError:\n",
    "        !pip install {pckg}\n",
    "        print(\"Successfully installed\", pckg)\n",
    "\n",
    "for el in packages_to_install:\n",
    "    install_packages(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "import torch\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "a7554158",
   "metadata": {},
   "source": [
    "# 1. Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8de153ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\sentiment-analysis-dataset\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import polars as pl\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset\")\n",
    "file =('sentiment-analysis-dataset/training.1600000.processed.noemoticon.csv')\n",
    "\n",
    "# Enter kaggle username and api key to download dataset. Api key can be found by following this link:\n",
    "# https://www.kaggle.com/settings/account\n",
    "\n",
    "df = pl.read_csv(file,separator=',', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "094e5578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "shape: (5, 6)\n┌─────────────┬─────────────────┬───────────────────┬──────────┬───────────────┬───────────────────┐\n│ polarity of ┆ id of the tweet ┆ date of the tweet ┆ query    ┆ user          ┆ text of the       │\n│ tweet       ┆ ---             ┆ ---               ┆ ---      ┆ ---           ┆ tweet             │\n│ ---         ┆ i64             ┆ str               ┆ str      ┆ str           ┆ ---               │\n│ i64         ┆                 ┆                   ┆          ┆               ┆ str               │\n╞═════════════╪═════════════════╪═══════════════════╪══════════╪═══════════════╪═══════════════════╡\n│ 0           ┆ 1467810672      ┆ Mon Apr 06        ┆ NO_QUERY ┆ scotthamilton ┆ is upset that he  │\n│             ┆                 ┆ 22:19:49 PDT 2009 ┆          ┆               ┆ can't update hi…  │\n│ 0           ┆ 1467810917      ┆ Mon Apr 06        ┆ NO_QUERY ┆ mattycus      ┆ @Kenichan I dived │\n│             ┆                 ┆ 22:19:53 PDT 2009 ┆          ┆               ┆ many times for…   │\n│ 0           ┆ 1467811184      ┆ Mon Apr 06        ┆ NO_QUERY ┆ ElleCTF       ┆ my whole body     │\n│             ┆                 ┆ 22:19:57 PDT 2009 ┆          ┆               ┆ feels itchy and   │\n│             ┆                 ┆                   ┆          ┆               ┆ li…               │\n│ 0           ┆ 1467811193      ┆ Mon Apr 06        ┆ NO_QUERY ┆ Karoli        ┆ @nationwideclass  │\n│             ┆                 ┆ 22:19:57 PDT 2009 ┆          ┆               ┆ no, it's not be…  │\n│ 0           ┆ 1467811372      ┆ Mon Apr 06        ┆ NO_QUERY ┆ joy_wolf      ┆ @Kwesidei not the │\n│             ┆                 ┆ 22:20:00 PDT 2009 ┆          ┆               ┆ whole crew        │\n└─────────────┴─────────────────┴───────────────────┴──────────┴───────────────┴───────────────────┘",
      "text/html": "<div><style>\n.dataframe > thead > tr > th,\n.dataframe > tbody > tr > td {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>polarity of tweet </th><th>id of the tweet</th><th>date of the tweet</th><th>query</th><th>user</th><th>text of the tweet </th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>1467810672</td><td>&quot;Mon Apr 06 22:…</td><td>&quot;NO_QUERY&quot;</td><td>&quot;scotthamilton&quot;</td><td>&quot;is upset that …</td></tr><tr><td>0</td><td>1467810917</td><td>&quot;Mon Apr 06 22:…</td><td>&quot;NO_QUERY&quot;</td><td>&quot;mattycus&quot;</td><td>&quot;@Kenichan I di…</td></tr><tr><td>0</td><td>1467811184</td><td>&quot;Mon Apr 06 22:…</td><td>&quot;NO_QUERY&quot;</td><td>&quot;ElleCTF&quot;</td><td>&quot;my whole body …</td></tr><tr><td>0</td><td>1467811193</td><td>&quot;Mon Apr 06 22:…</td><td>&quot;NO_QUERY&quot;</td><td>&quot;Karoli&quot;</td><td>&quot;@nationwidecla…</td></tr><tr><td>0</td><td>1467811372</td><td>&quot;Mon Apr 06 22:…</td><td>&quot;NO_QUERY&quot;</td><td>&quot;joy_wolf&quot;</td><td>&quot;@Kwesidei not …</td></tr></tbody></table></div>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd82a11",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e16f0",
   "metadata": {},
   "source": [
    "## 2.1 Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "348a71bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "shape: (5, 2)\n┌───────────┬───────────────────────────────────┐\n│ sentiment ┆ text                              │\n│ ---       ┆ ---                               │\n│ i64       ┆ str                               │\n╞═══════════╪═══════════════════════════════════╡\n│ 0         ┆ is upset that he can't update hi… │\n│ 0         ┆ @Kenichan I dived many times for… │\n│ 0         ┆ my whole body feels itchy and li… │\n│ 0         ┆ @nationwideclass no, it's not be… │\n│ 0         ┆ @Kwesidei not the whole crew      │\n└───────────┴───────────────────────────────────┘",
      "text/html": "<div><style>\n.dataframe > thead > tr > th,\n.dataframe > tbody > tr > td {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>text</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>&quot;is upset that …</td></tr><tr><td>0</td><td>&quot;@Kenichan I di…</td></tr><tr><td>0</td><td>&quot;my whole body …</td></tr><tr><td>0</td><td>&quot;@nationwidecla…</td></tr><tr><td>0</td><td>&quot;@Kwesidei not …</td></tr></tbody></table></div>"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.null_count()\n",
    "# No null data\n",
    "\n",
    "# Rename columns\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Drop columns that are not needed\n",
    "df = df.select(['sentiment', 'text'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec92584",
   "metadata": {},
   "source": [
    "## 2.2 Remove Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b80da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df = df.with_columns(pl.col('text').map_elements(lambda x: re.sub(r\"http\\S+\",\"\", x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e9d16",
   "metadata": {},
   "source": [
    "## 2.3 Remove punctuations and special characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d00a2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df = df.with_columns(pl.col('text').map_elements(lambda x: x.translate(str.maketrans('', '', string.punctuation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902784bd",
   "metadata": {},
   "source": [
    "## 2.4 Lowercase all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9249c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col('text').str.to_lowercase())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80899ed",
   "metadata": {},
   "source": [
    "## 2.5 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "086bf3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_columns(pl.col('text')\n",
    "                     .str.split(by=\" \")\n",
    "                    .alias('tokenized_text'))\n",
    "cleaned_df = df.drop('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1415a9ee",
   "metadata": {},
   "source": [
    "## 2.5 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e05513",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_df = cleaned_df.with_columns(pl.col('tokenized_text').map_elements(lambda x: [stemmer.stem(word) for word in x if (word and word not in get_stop_words('english'))]).alias('stemmed_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575959e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmed_df = stemmed_df.drop('tokenized_text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2495c96",
   "metadata": {},
   "source": [
    "# 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e677f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "stemmed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39fe381",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert polarity to sentiment\n",
    "# The polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "analysis_df = stemmed_df.clone()\n",
    "\n",
    "mapping = {0: 'negative', 2: 'neutral', 4: 'positive'}\n",
    "\n",
    "analysis_df = analysis_df.with_columns(pl.col('sentiment').map_dict(mapping).alias('sentiment'))\n",
    "\n",
    "sentiment_count =analysis_df.select(pl.col('sentiment').value_counts(sort=True)).unnest('sentiment')\n",
    "plt.barh(y=sentiment_count['sentiment'], width=sentiment_count['counts'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This indicates more tweets with a negative sentiments than positive sentiments. No tweets could be found with a neutral sentiment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Frequency of tokenized words\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def count_word_freq(column):\n",
    "    return Counter([item for sublist in column.to_list() for item in sublist])\n",
    "\n",
    "freq = count_word_freq(stemmed_df.get_column('stemmed_text'))\n",
    "freq_negative_sentiment = count_word_freq(stemmed_df.filter(pl.col('sentiment') == 'negative').get_column('stemmed_text'))\n",
    "freq_positive_sentiment = count_word_freq(stemmed_df.filter(pl.col('sentiment') == 'positive').get_column('stemmed_text'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize top 50 frequently used words\n",
    "word, frequency = zip(*freq.most_common(n=50))\n",
    "indices = np.arange(len(freq.most_common(n=50)))\n",
    "plt.bar(indices, frequency, color='b')\n",
    "plt.xticks(indices, word, rotation='vertical')\n",
    "plt.title('Top 50 frequently used words')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create wordcloud\n",
    "import wordcloud\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "pos_cloud = wordcloud.WordCloud(max_words=150, random_state=30, collocations=True)\n",
    "neg_cloud = wordcloud.WordCloud(max_words=150, random_state=30, collocations=True)\n",
    "negative_sentiment_cloud = neg_cloud.generate_from_frequencies(dict(freq_negative_sentiment.most_common()))\n",
    "positive_sentiment_cloud = pos_cloud.generate_from_frequencies(dict(freq_positive_sentiment.most_common()))\n",
    "\n",
    "ax1.imshow(negative_sentiment_cloud, interpolation='bilinear')\n",
    "ax1.set_title('Negative Sentiment Words')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(positive_sentiment_cloud, interpolation='bilinear')\n",
    "ax2.set_title('Positive Sentiment Words')\n",
    "ax2.axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches(15,15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Text Vectorization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create model for word vectorization\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "X = stemmed_df.select('stemmed_text')\n",
    "y = stemmed_df.select('sentiment')\n",
    "\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    vector_size = 100\n",
    "\n",
    "    new_fasttext_model = FastText(vector_size= vector_size, window= 3, min_count= 1, sg=1)\n",
    "    new_fasttext_model.build_vocab(X.to_series().to_list())\n",
    "    new_fasttext_model.train(X.to_series().to_list(), total_examples=new_fasttext_model.corpus_count, epochs=5)\n",
    "\n",
    "    new_fasttext_model.save(\"fasttext.model\")\n",
    "\n",
    "    return new_fasttext_model\n",
    "\n",
    "def get_fasttext_model():\n",
    "    try:\n",
    "        pretrained_model = FastText.load(\"fasttext.model\")\n",
    "        print(\"Successfully loaded pretrained model\")\n",
    "        return pretrained_model\n",
    "    except AttributeError:\n",
    "        print(\"Training new model\")\n",
    "        return train_model()\n",
    "\n",
    "fasttext_model = get_fasttext_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.to_series().to_list(), y.to_series().to_list(), test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "X_train_vectors = []\n",
    "for sentence in X_train:\n",
    "    sentence_vectors = []\n",
    "    for token in sentence:\n",
    "        try:\n",
    "            word_vector = fasttext_model.wv[token]\n",
    "            sentence_vectors.append(word_vector)\n",
    "        except KeyError:\n",
    "            # Handle out-of-vocabulary words here, e.g., replace with a placeholder vector\n",
    "            pass\n",
    "\n",
    "    if len(sentence_vectors) > 0:\n",
    "        # Calculate the average of word vectors for the sentence\n",
    "        sentence_vector = np.mean(sentence_vectors, axis=0)\n",
    "        X_train_vectors.append(sentence_vector)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "X_test_vectors = []\n",
    "for sentence in X_test:\n",
    "    sentence_vectors = []\n",
    "    for token in sentence:\n",
    "        try:\n",
    "            word_vector = fasttext_model.wv[token]\n",
    "            sentence_vectors.append(word_vector)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if len(sentence_vectors) > 0:\n",
    "        sentence_vector = np.mean(sentence_vectors, axis=0)\n",
    "        X_test_vectors.append(sentence_vector)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(X_train_vectors[1])\n",
    "print(X_test_vectors[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Model (BERT)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "X_train_tensors = torch.tensor(X_train_vectors)\n",
    "X_test_tensors = torch.tensor(X_test_vectors)\n",
    "y_train_tensors = torch.tensor(y_train)\n",
    "y_test_tensors = torch.tensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
